services:
  chat-proxy:
    build:
      context: .
      dockerfile: Dockerfile.chat-proxy
    container_name: imageworks-chat-proxy
    restart: unless-stopped
    gpus: all
    environment:
      - CHAT_PROXY_HOST=0.0.0.0
      - CHAT_PROXY_PORT=8100
      - CHAT_PROXY_SUPPRESS_DECORATIONS=1
      - CHAT_PROXY_INCLUDE_NON_INSTALLED=0
      - CHAT_PROXY_AUTOSTART_ENABLED=1
      - CHAT_PROXY_OLLAMA_BASE_URL=http://imageworks-ollama:11434
    ports:
      - "8100:8100"
    volumes:
      - ./configs:/app/configs
      - ./logs:/app/logs
      - ./models:/app/models:ro
      - ./outputs:/app/outputs
      - ./src:/app/src:ro
      - ./scripts:/app/scripts:ro
      - ./_staging:/app/_staging
      - /home/stewa/ai-models/weights:/home/stewa/ai-models/weights:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: ["uvicorn", "imageworks.chat_proxy.app:app", "--host", "0.0.0.0", "--port", "8100"]

  ollama:
    image: ollama/ollama:latest
    container_name: imageworks-ollama
    restart: unless-stopped
    gpus: all
    environment:
      - OLLAMA_ORIGINS=*
    volumes:
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
