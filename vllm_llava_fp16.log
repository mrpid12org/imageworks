/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 23:37:19 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=1020428)[0;0m INFO 09-30 23:37:20 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=1020428)[0;0m INFO 09-30 23:37:20 [utils.py:328] non-default args: {'model_tag': 'llava-hf/llava-1.5-7b-hf', 'host': '0.0.0.0', 'port': 24001, 'model': 'llava-hf/llava-1.5-7b-hf', 'dtype': 'float16', 'max_model_len': 2048}
[1;36m(APIServer pid=1020428)[0;0m INFO 09-30 23:37:26 [__init__.py:742] Resolved architecture: LlavaForConditionalGeneration
[1;36m(APIServer pid=1020428)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1020428)[0;0m INFO 09-30 23:37:26 [__init__.py:1815] Using max model len 2048
[1;36m(APIServer pid=1020428)[0;0m INFO 09-30 23:37:26 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=1020428)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(APIServer pid=1020428)[0;0m   warnings.warn(msg, FutureWarning)
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 23:37:30 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:31 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:31 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1020571)[0;0m WARNING 09-30 23:37:32 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[W930 23:37:32.350472793 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1020571)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   warnings.warn(msg, FutureWarning)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m WARNING 09-30 23:37:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 67108.86it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:35 [gpu_model_runner.py:2338] Starting to load model llava-hf/llava-1.5-7b-hf...
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:35 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:35 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:35 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.55it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.51it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.37it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.41it/s]
[1;36m(EngineCore_DP0 pid=1020571)[0;0m
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:38 [default_loader.py:268] Loading weights took 2.28 seconds
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:38 [gpu_model_runner.py:2392] Model loading took 13.1343 GiB and 3.273683 seconds
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:38 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 3 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:41 [backends.py:539] Using cache directory: /home/stewa/.cache/vllm/torch_compile_cache/bdd9557a60/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:41 [backends.py:550] Dynamo bytecode transform time: 2.69 s
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:42 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:44 [backends.py:215] Compiling a graph for dynamic shape takes 2.16 s
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:44 [monitor.py:34] torch.compile takes 4.85 s in total
[1;36m(EngineCore_DP0 pid=1020571)[0;0m INFO 09-30 23:37:45 [gpu_worker.py:298] Available KV cache memory: 0.26 GiB
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 708, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718]     raise ValueError(
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ERROR 09-30 23:37:45 [core.py:718] ValueError: To serve at least one request with the models's max seq len (2048), (1.00 GiB KV cache is needed, which is larger than the available KV cache memory (0.26 GiB). Based on the available memory, the estimated maximum model length is 512. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=1020571)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 722, in run_engine_core
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=1020571)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 708, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=1020571)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=1020571)[0;0m ValueError: To serve at least one request with the models's max seq len (2048), (1.00 GiB KV cache is needed, which is larger than the available KV cache memory (0.26 GiB). Based on the available memory, the estimated maximum model length is 512. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W930 23:37:45.899269707 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=1020428)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/bin/vllm", line 10, in <module>
[1;36m(APIServer pid=1020428)[0;0m     sys.exit(main())
[1;36m(APIServer pid=1020428)[0;0m              ^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 54, in main
[1;36m(APIServer pid=1020428)[0;0m     args.dispatch_function(args)
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 50, in cmd
[1;36m(APIServer pid=1020428)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=1020428)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
[1;36m(APIServer pid=1020428)[0;0m     return runner.run(main)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=1020428)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=1020428)[0;0m     return await main
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1941, in run_server
[1;36m(APIServer pid=1020428)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1961, in run_server_worker
[1;36m(APIServer pid=1020428)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=1020428)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1020428)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 179, in build_async_engine_client
[1;36m(APIServer pid=1020428)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=1020428)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1020428)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 221, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=1020428)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=1020428)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 1589, in inner
[1;36m(APIServer pid=1020428)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
[1;36m(APIServer pid=1020428)[0;0m     return cls(
[1;36m(APIServer pid=1020428)[0;0m            ^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
[1;36m(APIServer pid=1020428)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=1020428)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=1020428)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=1020428)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 769, in __init__
[1;36m(APIServer pid=1020428)[0;0m     super().__init__(
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
[1;36m(APIServer pid=1020428)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=1020428)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=1020428)[0;0m     next(self.gen)
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
[1;36m(APIServer pid=1020428)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=1020428)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
[1;36m(APIServer pid=1020428)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=1020428)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
