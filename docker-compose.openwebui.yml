services:
  chat-proxy:
    build:
      context: .
      dockerfile: Dockerfile.chat-proxy
    container_name: imageworks-chat-proxy
    restart: unless-stopped
    environment:
      CHAT_PROXY_AUTOSTART_ENABLED: "false"
      CHAT_PROXY_AUTOSTART_MAP: |
        {"qwen2.5-vl-7b-instruct_(AWQ)":{"command":["python","scripts/start_vllm_server.py","--model-name","qwen2.5-vl-7b-instruct_(AWQ)","--background","--log-file","/app/logs/autostart-qwen2_5-vl-7b-instruct__AWQ_.log"]},"deepseek-coder-6.7b-instruct_(AWQ)":{"command":["python","scripts/start_vllm_server.py","--model-name","deepseek-coder-6.7b-instruct_(AWQ)","--background","--log-file","/app/logs/autostart-deepseek-coder-6_7b-instruct__AWQ_.log"]},"codellama-13b-instruct_(AWQ)":{"command":["python","scripts/start_vllm_server.py","--model-name","codellama-13b-instruct_(AWQ)","--background","--log-file","/app/logs/autostart-codellama-13b-instruct__AWQ_.log"]},"gpt-oss-20b_(MXFP4)":{"command":["python","scripts/start_vllm_server.py","--model-name","gpt-oss-20b_(MXFP4)","--background","--log-file","/app/logs/autostart-gpt-oss-20b__MXFP4_.log"]},"dialogpt-small":{"command":["python","scripts/start_vllm_server.py","--model-name","dialogpt-small","--background","--log-file","/app/logs/autostart-dialogpt-small.log"]},"qwen2.5-coder-7b-instruct_(GPTQ_INT8)":{"command":["python","scripts/start_vllm_server.py","--model-name","qwen2.5-coder-7b-instruct_(GPTQ_INT8)","--background","--log-file","/app/logs/autostart-qwen2_5-coder-7b-instruct__GPTQ_INT8_.log"]},"qwen2.5-vl-7b-instruct_(BF16)":{"command":["python","scripts/start_vllm_server.py","--model-name","qwen2.5-vl-7b-instruct_(BF16)","--background","--log-file","/app/logs/autostart-qwen2_5-vl-7b-instruct__BF16_.log"]},"siglip-large-patch16-384_(FP32)":{"command":["python","scripts/start_vllm_server.py","--model-name","siglip-large-patch16-384_(FP32)","--background","--log-file","/app/logs/autostart-siglip-large-patch16-384__FP32_.log"]}}
      CHAT_PROXY_HOST: "0.0.0.0"
      CHAT_PROXY_PORT: "8100"
      CHAT_PROXY_SUPPRESS_DECORATIONS: "1"
      CHAT_PROXY_INCLUDE_NON_INSTALLED: "0"
      CHAT_PROXY_LOOPBACK_ALIAS: "host.docker.internal"
    ports:
      - "8100:8100"  # Host:Container
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8100/v1/health"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - ./configs:/app/configs:ro
      - ./logs:/app/logs
      - ./models:/models:ro
      - ./outputs:/outputs
      - ./src:/app/src:ro
      - ./scripts:/app/scripts:ro
      - ./_staging:/app/_staging
      # Mount HF weights root at the same absolute path used on host
      - /home/stewa/ai-models/weights:/home/stewa/ai-models/weights:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - imageworks-net
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"  # Host:Container
    environment:
      # Point OpenWebUI at ImageWorks chat proxy (adjust if proxy runs elsewhere)
      - OPENAI_API_BASE_URL=http://chat-proxy:8100/v1
      # Optionally set an API key placeholder if UI demands one
      - OPENAI_API_KEY=EMPTY
      # Disable Ollama provider to avoid duplicate model sources in the UI
      - ENABLE_OLLAMA_API=false
      # Also ensure no Ollama endpoints are pre-configured
      - OLLAMA_BASE_URLS=
      - OLLAMA_API_CONFIGS=
      # Reset config on start so env vars above take precedence over DB defaults
      - RESET_CONFIG_ON_START=true
    depends_on:
      chat-proxy:
        condition: service_healthy
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - imageworks-net
    # GPU acceleration (NVIDIA). Requires NVIDIA Container Toolkit installed on host.
    gpus: all
    # If using WSL2 and host.docker.internal is unreliable, expose network_mode host (Linux only)
    # network_mode: host
    # Or define an explicit network shared with proxy
volumes:
  openwebui-data:

networks:
  imageworks-net:
    driver: bridge
