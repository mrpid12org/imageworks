services:
  chat-proxy:
    build:
      context: .
      dockerfile: Dockerfile.chat-proxy
    container_name: imageworks-chat-proxy
    restart: unless-stopped
    environment:
      CHAT_PROXY_AUTOSTART_ENABLED: "1"
      CHAT_PROXY_HOST: "0.0.0.0"
      CHAT_PROXY_PORT: "8100"
      CHAT_PROXY_SUPPRESS_DECORATIONS: "1"
      CHAT_PROXY_INCLUDE_NON_INSTALLED: "0"
      CHAT_PROXY_LOOPBACK_ALIAS: ""
      CHAT_PROXY_AUTOSTART_GRACE_PERIOD_S: "45"
      CHAT_PROXY_VLLM_SINGLE_PORT: "${CHAT_PROXY_VLLM_SINGLE_PORT:-1}"
      CHAT_PROXY_VLLM_PORT: "${CHAT_PROXY_VLLM_PORT:-24001}"
      CHAT_PROXY_VLLM_STATE_PATH: "${CHAT_PROXY_VLLM_STATE_PATH:-/app/_staging/active_vllm.json}"
      CHAT_PROXY_VLLM_GPU_MEMORY_UTILIZATION: "${CHAT_PROXY_VLLM_GPU_MEMORY_UTILIZATION:-0.7}"
      CHAT_PROXY_VLLM_MAX_MODEL_LEN: "${CHAT_PROXY_VLLM_MAX_MODEL_LEN:-8192}"
    ports:
      - "8100:8100"  # Host:Container
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8100/v1/health"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - ./configs:/app/configs:ro
      - ./logs:/app/logs
      - ./models:/models:ro
      - ./outputs:/outputs
      - ./src:/app/src:ro
      - ./scripts:/app/scripts:ro
      - ./_staging:/app/_staging
      # Mount HF weights root at the same absolute path used on host
      - /home/stewa/ai-models/weights:/home/stewa/ai-models/weights:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    gpus: all
    networks:
      - imageworks-net
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"  # Host:Container
    environment:
      # Point OpenWebUI at ImageWorks chat proxy (adjust if proxy runs elsewhere)
      - OPENAI_API_BASE_URL=http://chat-proxy:8100/v1
      # Optionally set an API key placeholder if UI demands one
      - OPENAI_API_KEY=EMPTY
      # Disable Ollama provider to avoid duplicate model sources in the UI
      - ENABLE_OLLAMA_API=false
      # Also ensure no Ollama endpoints are pre-configured
      - OLLAMA_BASE_URLS=
      - OLLAMA_API_CONFIGS=
      # Reset config on start so env vars above take precedence over DB defaults
      - RESET_CONFIG_ON_START=true
    depends_on:
      chat-proxy:
        condition: service_healthy
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - imageworks-net
    # GPU acceleration (NVIDIA). Requires NVIDIA Container Toolkit installed on host.
    gpus: all
    # If using WSL2 and host.docker.internal is unreliable, expose network_mode host (Linux only)
    # network_mode: host
    # Or define an explicit network shared with proxy
volumes:
  openwebui-data:

networks:
  imageworks-net:
    external: true
    name: imageworks_default
