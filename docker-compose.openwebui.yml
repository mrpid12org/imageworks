services:
  chat-proxy:
    build:
      context: .
      dockerfile: Dockerfile.chat-proxy
    container_name: imageworks-chat-proxy
    restart: unless-stopped
    environment:
      CHAT_PROXY_AUTOSTART_ENABLED: "true"
      CHAT_PROXY_AUTOSTART_MAP: |
        {"qwen2.5-vl-7b-instruct_(AWQ)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "qwen2.5-vl-7b-instruct_(AWQ)"]}, "deepseek-coder-6.7b-instruct_(AWQ)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "deepseek-coder-6.7b-instruct_(AWQ)"]}, "codellama-13b-instruct_(AWQ)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "codellama-13b-instruct_(AWQ)"]}, "gpt-oss-20b_(MXFP4)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "gpt-oss-20b_(MXFP4)"]}, "dialogpt-small": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "dialogpt-small"]}, "qwen2.5-coder-7b-instruct_(GPTQ_INT8)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "qwen2.5-coder-7b-instruct_(GPTQ_INT8)"]}, "qwen2.5-vl-7b-instruct_(BF16)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "qwen2.5-vl-7b-instruct_(BF16)"]}, "siglip-large-patch16-384_(FP32)": {"command": ["python", "scripts/start_vllm_server.py", "--model-name", "siglip-large-patch16-384_(FP32)"]}}
      CHAT_PROXY_HOST: "0.0.0.0"
      CHAT_PROXY_PORT: "8100"
      CHAT_PROXY_SUPPRESS_DECORATIONS: "1"
      CHAT_PROXY_INCLUDE_NON_INSTALLED: "0"
    ports:
      - "8100:8100"  # Host:Container
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8100/v1/health"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - ./configs:/app/configs:ro
      - ./models:/models:ro
      - ./outputs:/outputs
      # Mount HF weights root at the same absolute path used on host
      - /home/stewa/ai-models/weights:/home/stewa/ai-models/weights:ro
    networks:
      - imageworks-net
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"  # Host:Container
    environment:
      # Point OpenWebUI at ImageWorks chat proxy (adjust if proxy runs elsewhere)
      - OPENAI_API_BASE_URL=http://chat-proxy:8100/v1
      # Optionally set an API key placeholder if UI demands one
      - OPENAI_API_KEY=EMPTY
      # Disable Ollama provider to avoid duplicate model sources in the UI
      - ENABLE_OLLAMA_API=false
      # Also ensure no Ollama endpoints are pre-configured
      - OLLAMA_BASE_URLS=
      - OLLAMA_API_CONFIGS=
      # Reset config on start so env vars above take precedence over DB defaults
      - RESET_CONFIG_ON_START=true
    depends_on:
      chat-proxy:
        condition: service_healthy
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - imageworks-net
    # GPU acceleration (NVIDIA). Requires NVIDIA Container Toolkit installed on host.
    gpus: all
    # If using WSL2 and host.docker.internal is unreliable, expose network_mode host (Linux only)
    # network_mode: host
    # Or define an explicit network shared with proxy
volumes:
  openwebui-data:

networks:
  imageworks-net:
    driver: bridge
