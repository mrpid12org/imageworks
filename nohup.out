/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 18:28:54 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:54 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:54 [utils.py:328] non-default args: {'host': 'localhost', 'port': 24001, 'model': '/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', 'trust_remote_code': True, 'max_model_len': 4096, 'served_model_name': ['llava-1.5-7b-hf-awq'], 'gpu_memory_utilization': 0.85}
[1;36m(APIServer pid=941339)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:58 [__init__.py:742] Resolved architecture: LlavaForConditionalGeneration
[1;36m(APIServer pid=941339)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:58 [__init__.py:1815] Using max model len 4096
[1;36m(APIServer pid=941339)[0;0m WARNING 09-30 18:28:59 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:59 [awq_marlin.py:117] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[1;36m(APIServer pid=941339)[0;0m INFO 09-30 18:28:59 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=941339)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(APIServer pid=941339)[0;0m   warnings.warn(msg, FutureWarning)
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 18:29:02 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=941609)[0;0m INFO 09-30 18:29:03 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=941609)[0;0m INFO 09-30 18:29:03 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', speculative_config=None, tokenizer='/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-1.5-7b-hf-awq, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=941609)[0;0m WARNING 09-30 18:29:03 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 82, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     self._init_executor()
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     self.collective_rpc("init_device")
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3060, in run_method
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in init_device
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 180, in init_device
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718]     raise ValueError(
[1;36m(EngineCore_DP0 pid=941609)[0;0m ERROR 09-30 18:29:03 [core.py:718] ValueError: Free memory on device (13.58/15.99 GiB) on startup is less than desired GPU memory utilization (0.85, 13.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=941609)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=941609)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 722, in run_engine_core
[1;36m(EngineCore_DP0 pid=941609)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=941609)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 82, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=941609)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.collective_rpc("init_device")
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_DP0 pid=941609)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3060, in run_method
[1;36m(EngineCore_DP0 pid=941609)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=941609)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in init_device
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=941609)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 180, in init_device
[1;36m(EngineCore_DP0 pid=941609)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=941609)[0;0m ValueError: Free memory on device (13.58/15.99 GiB) on startup is less than desired GPU memory utilization (0.85, 13.59 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=941609)[0;0m Exception ignored in: <function ExecutorBase.__del__ at 0x7a42bd9a3420>
[1;36m(EngineCore_DP0 pid=941609)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 237, in __del__
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.shutdown()
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 76, in shutdown
[1;36m(EngineCore_DP0 pid=941609)[0;0m     worker.shutdown()
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 528, in shutdown
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.worker.shutdown()
[1;36m(EngineCore_DP0 pid=941609)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 675, in shutdown
[1;36m(EngineCore_DP0 pid=941609)[0;0m     self.model_runner.ensure_kv_transfer_shutdown()
[1;36m(EngineCore_DP0 pid=941609)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=941609)[0;0m AttributeError: 'NoneType' object has no attribute 'ensure_kv_transfer_shutdown'
[1;36m(APIServer pid=941339)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=941339)[0;0m   File "<frozen runpy>", line 198, in _run_module_as_main
[1;36m(APIServer pid=941339)[0;0m   File "<frozen runpy>", line 88, in _run_code
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 2011, in <module>
[1;36m(APIServer pid=941339)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=941339)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
[1;36m(APIServer pid=941339)[0;0m     return runner.run(main)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=941339)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=941339)[0;0m     return await main
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1941, in run_server
[1;36m(APIServer pid=941339)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1961, in run_server_worker
[1;36m(APIServer pid=941339)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=941339)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=941339)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 179, in build_async_engine_client
[1;36m(APIServer pid=941339)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=941339)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=941339)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 221, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=941339)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=941339)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 1589, in inner
[1;36m(APIServer pid=941339)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
[1;36m(APIServer pid=941339)[0;0m     return cls(
[1;36m(APIServer pid=941339)[0;0m            ^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
[1;36m(APIServer pid=941339)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=941339)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=941339)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=941339)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 769, in __init__
[1;36m(APIServer pid=941339)[0;0m     super().__init__(
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
[1;36m(APIServer pid=941339)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=941339)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=941339)[0;0m     next(self.gen)
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
[1;36m(APIServer pid=941339)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=941339)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
[1;36m(APIServer pid=941339)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=941339)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
vLLM server exited with status 1
🚀 Launching vLLM server with command:
python -m vllm.entrypoints.openai.api_server --model /home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq --host localhost --port 24001 --served-model-name llava-1.5-7b-hf-awq --tensor-parallel-size 1 --gpu-memory-utilization 0.85 --max-model-len 4096 --dtype auto --trust-remote-code
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 18:36:41 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:42 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:42 [utils.py:328] non-default args: {'host': 'localhost', 'port': 24001, 'chat_template': "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{{ system_message }}{% endif %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + ' ASSISTANT:' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + '</s>' }}{% endif %}{% endfor %}", 'model': '/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', 'trust_remote_code': True, 'max_model_len': 4096, 'served_model_name': ['llava-1.5-7b-hf-awq'], 'gpu_memory_utilization': 0.85}
[1;36m(APIServer pid=943457)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:47 [__init__.py:742] Resolved architecture: LlavaForConditionalGeneration
[1;36m(APIServer pid=943457)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:47 [__init__.py:1815] Using max model len 4096
[1;36m(APIServer pid=943457)[0;0m WARNING 09-30 18:36:47 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:47 [awq_marlin.py:117] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:36:47 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=943457)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(APIServer pid=943457)[0;0m   warnings.warn(msg, FutureWarning)
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 09-30 18:36:50 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:51 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:51 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', speculative_config=None, tokenizer='/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-1.5-7b-hf-awq, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=943603)[0;0m WARNING 09-30 18:36:51 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[W930 18:36:52.789842130 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:52 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=943603)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(EngineCore_DP0 pid=943603)[0;0m   warnings.warn(msg, FutureWarning)
[1;36m(EngineCore_DP0 pid=943603)[0;0m WARNING 09-30 18:36:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=943603)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:53 [gpu_model_runner.py:2338] Starting to load model /home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq...
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:53 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:53 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=943603)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=943603)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
[1;36m(EngineCore_DP0 pid=943603)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
[1;36m(EngineCore_DP0 pid=943603)[0;0m
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:54 [default_loader.py:268] Loading weights took 0.74 seconds
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:54 [gpu_model_runner.py:2392] Model loading took 4.2530 GiB and 1.175218 seconds
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:54 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 3 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:58 [backends.py:539] Using cache directory: /home/stewa/.cache/vllm/torch_compile_cache/be651e7334/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:36:58 [backends.py:550] Dynamo bytecode transform time: 3.55 s
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.270 s
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:00 [monitor.py:34] torch.compile takes 3.55 s in total
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:01 [gpu_worker.py:298] Available KV cache memory: 8.99 GiB
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:01 [kv_cache_utils.py:864] GPU KV cache size: 18,400 tokens
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:01 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 4.49x
[1;36m(EngineCore_DP0 pid=943603)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 2/67 [00:00<00:05, 11.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:05, 11.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 6/67 [00:00<00:05, 11.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:04, 12.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|█▍        | 10/67 [00:00<00:04, 12.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:04, 12.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 14/67 [00:01<00:04, 12.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 16/67 [00:01<00:03, 13.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:01<00:03, 13.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|██▉       | 20/67 [00:01<00:03, 13.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 22/67 [00:01<00:03, 13.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 24/67 [00:01<00:03, 14.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 26/67 [00:01<00:02, 14.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:02<00:02, 15.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:02<00:02, 15.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|████▊     | 32/67 [00:02<00:02, 16.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 34/67 [00:02<00:01, 16.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:02<00:01, 17.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:02<00:01, 17.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 41/67 [00:02<00:01, 18.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 44/67 [00:02<00:01, 19.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|███████   | 47/67 [00:03<00:00, 20.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 50/67 [00:03<00:00, 21.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:03<00:00, 22.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▎ | 56/67 [00:03<00:00, 24.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|████████▉ | 60/67 [00:03<00:00, 27.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 64/67 [00:03<00:00, 30.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 18.08it/s]
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:05 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 1.27 GiB
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:05 [gpu_worker.py:391] Free memory on device (14.66/15.99 GiB) on startup. Desired GPU memory utilization is (0.85, 13.59 GiB). Actual usage is 4.25 GiB for weight, 0.34 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.27 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=8131478732` to fit into requested memory, or `--kv-cache-memory=9277388800` to fully utilize gpu memory. Current kv cache memory in use is 9649816780 bytes.
[1;36m(EngineCore_DP0 pid=943603)[0;0m INFO 09-30 18:37:05 [core.py:218] init engine (profile, create kv cache, warmup model) took 11.23 seconds
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1150
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [api_server.py:1692] Supported_tasks: ['generate']
[1;36m(APIServer pid=943457)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(APIServer pid=943457)[0;0m WARNING 09-30 18:37:06 [api_server.py:1712] Using supplied chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{{ system_message }}{% endif %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + ' ASSISTANT:' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + '</s>' }}{% endif %}{% endfor %}
[1;36m(APIServer pid=943457)[0;0m WARNING 09-30 18:37:06 [api_server.py:1712] It is different from official chat template '/home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq'. This discrepancy may lead to performance degradation.
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [api_server.py:1971] Starting vLLM API server 0 on http://localhost:24001
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:06 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=943457)[0;0m INFO:     Started server process [943457]
[1;36m(APIServer pid=943457)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=943457)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48268 - "GET /v1/models HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:37 [chat_utils.py:538] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:47 [loggers.py:123] Engine 000: Avg prompt throughput: 131.5 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:37:57 [loggers.py:123] Engine 000: Avg prompt throughput: 210.7 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.9%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:07 [loggers.py:123] Engine 000: Avg prompt throughput: 291.1 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 1.5%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:17 [loggers.py:123] Engine 000: Avg prompt throughput: 210.8 tokens/s, Avg generation throughput: 111.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.7%, Prefix cache hit rate: 1.7%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:28 [loggers.py:123] Engine 000: Avg prompt throughput: 275.4 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:38 [loggers.py:123] Engine 000: Avg prompt throughput: 278.0 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:48 [loggers.py:123] Engine 000: Avg prompt throughput: 291.7 tokens/s, Avg generation throughput: 111.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 2.0%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:38:59 [loggers.py:123] Engine 000: Avg prompt throughput: 211.1 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 2.0%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:09 [loggers.py:123] Engine 000: Avg prompt throughput: 274.9 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:19 [loggers.py:123] Engine 000: Avg prompt throughput: 277.1 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:29 [loggers.py:123] Engine 000: Avg prompt throughput: 291.9 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:39 [loggers.py:123] Engine 000: Avg prompt throughput: 210.7 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.3%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:49 [loggers.py:123] Engine 000: Avg prompt throughput: 275.7 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:39:59 [loggers.py:123] Engine 000: Avg prompt throughput: 276.9 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:40:09 [loggers.py:123] Engine 000: Avg prompt throughput: 291.3 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 2.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:40:19 [loggers.py:123] Engine 000: Avg prompt throughput: 211.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:40:29 [loggers.py:123] Engine 000: Avg prompt throughput: 211.0 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.1%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:40:39 [loggers.py:123] Engine 000: Avg prompt throughput: 340.9 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:44706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:40:50 [loggers.py:123] Engine 000: Avg prompt throughput: 80.4 tokens/s, Avg generation throughput: 76.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:46488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:46488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:00 [loggers.py:123] Engine 000: Avg prompt throughput: 130.6 tokens/s, Avg generation throughput: 38.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:46488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:10 [loggers.py:123] Engine 000: Avg prompt throughput: 81.1 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.2%
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:20 [loggers.py:123] Engine 000: Avg prompt throughput: 64.3 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 3.4%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:31 [loggers.py:123] Engine 000: Avg prompt throughput: 277.0 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 5.9%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:41 [loggers.py:123] Engine 000: Avg prompt throughput: 290.6 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 5.7%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:41:51 [loggers.py:123] Engine 000: Avg prompt throughput: 210.3 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 5.5%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:01 [loggers.py:123] Engine 000: Avg prompt throughput: 275.6 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 5.4%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:11 [loggers.py:123] Engine 000: Avg prompt throughput: 277.3 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 5.2%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:21 [loggers.py:123] Engine 000: Avg prompt throughput: 291.2 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 5.1%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:32 [loggers.py:123] Engine 000: Avg prompt throughput: 210.9 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 5.0%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:42 [loggers.py:123] Engine 000: Avg prompt throughput: 211.5 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 4.9%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:42:52 [loggers.py:123] Engine 000: Avg prompt throughput: 275.3 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 4.8%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:02 [loggers.py:123] Engine 000: Avg prompt throughput: 277.2 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 4.7%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:12 [loggers.py:123] Engine 000: Avg prompt throughput: 291.2 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 4.7%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:22 [loggers.py:123] Engine 000: Avg prompt throughput: 211.1 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 4.6%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:32 [loggers.py:123] Engine 000: Avg prompt throughput: 210.5 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 4.5%
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO:     127.0.0.1:48038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:43 [loggers.py:123] Engine 000: Avg prompt throughput: 211.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 4.5%
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:43:53 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 4.5%
[1;36m(APIServer pid=943457)[0;0m WARNING 09-30 18:51:40 [launcher.py:98] port 24001 is used by process psutil.Process(pid=943457, name='python3', status='running') launched with command:
[1;36m(APIServer pid=943457)[0;0m WARNING 09-30 18:51:40 [launcher.py:98] /home/stewa/code/imageworks/.venv/bin/python3 -m vllm.entrypoints.openai.api_server --model /home/stewa/ai-models/weights/ybelkada/llava-1.5-7b-hf-awq --host localhost --port 24001 --served-model-name llava-1.5-7b-hf-awq --tensor-parallel-size 1 --gpu-memory-utilization 0.85 --max-model-len 4096 --dtype auto --trust-remote-code --chat-template {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{{ system_message }}{% endif %}{% if message['role'] == 'user' %}{{ 'USER: ' + message['content'] + ' ASSISTANT:' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + '</s>' }}{% endif %}{% endfor %}
[1;36m(APIServer pid=943457)[0;0m INFO 09-30 18:51:40 [launcher.py:101] Shutting down FastAPI HTTP server.
[rank0]:[W930 18:51:40.379017585 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=943457)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=943457)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=943457)[0;0m INFO:     Application shutdown complete.
