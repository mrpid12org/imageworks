/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 10-01 23:05:33 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:34 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:34 [utils.py:328] non-default args: {'host': '0.0.0.0', 'port': 24005, 'chat_template': 'llava15_vicuna.jinja', 'model': '/home/stewa/ai-models/weights/llava-hf/LLaVA-NeXT-Video-7B-hf', 'trust_remote_code': True, 'max_model_len': 4096, 'enforce_eager': True, 'served_model_name': ['llava-next-video-7b'], 'gpu_memory_utilization': 0.83}
[1;36m(APIServer pid=1193150)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:38 [__init__.py:742] Resolved architecture: LlavaNextVideoForConditionalGeneration
[1;36m(APIServer pid=1193150)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:38 [__init__.py:1815] Using max model len 4096
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:39 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=1193150)[0;0m INFO 10-01 23:05:39 [__init__.py:3400] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=1193150)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(APIServer pid=1193150)[0;0m   warnings.warn(msg, FutureWarning)
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 10-01 23:05:42 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:43 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/home/stewa/ai-models/weights/llava-hf/LLaVA-NeXT-Video-7B-hf', speculative_config=None, tokenizer='/home/stewa/ai-models/weights/llava-hf/LLaVA-NeXT-Video-7B-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-next-video-7b, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1193328)[0;0m WARNING 10-01 23:05:44 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[W1001 23:05:44.473651490 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:45 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1193328)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   warnings.warn(msg, FutureWarning)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m WARNING 10-01 23:05:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:46 [gpu_model_runner.py:2338] Starting to load model /home/stewa/ai-models/weights/llava-hf/LLaVA-NeXT-Video-7B-hf...
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:46 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:46 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:46 [__init__.py:3400] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.33s/it]
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.38s/it]
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.49s/it]
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.46s/it]
[1;36m(EngineCore_DP0 pid=1193328)[0;0m
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:51 [default_loader.py:268] Loading weights took 4.42 seconds
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:51 [gpu_model_runner.py:2392] Model loading took 13.1357 GiB and 4.761309 seconds
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:51 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 4032 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m INFO 10-01 23:05:53 [gpu_worker.py:298] Available KV cache memory: -1.66 GiB
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 691, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718]     raise ValueError("No available memory for the cache blocks. "
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ERROR 10-01 23:05:53 [core.py:718] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=1193328)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 722, in run_engine_core
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 193, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1110, in get_kv_cache_config
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
[1;36m(EngineCore_DP0 pid=1193328)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 691, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=1193328)[0;0m     raise ValueError("No available memory for the cache blocks. "
[1;36m(EngineCore_DP0 pid=1193328)[0;0m ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
[rank0]:[W1001 23:05:54.764242756 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=1193150)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=1193150)[0;0m   File "<frozen runpy>", line 198, in _run_module_as_main
[1;36m(APIServer pid=1193150)[0;0m   File "<frozen runpy>", line 88, in _run_code
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 2011, in <module>
[1;36m(APIServer pid=1193150)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=1193150)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
[1;36m(APIServer pid=1193150)[0;0m     return runner.run(main)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=1193150)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=1193150)[0;0m     return await main
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1941, in run_server
[1;36m(APIServer pid=1193150)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1961, in run_server_worker
[1;36m(APIServer pid=1193150)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=1193150)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1193150)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 179, in build_async_engine_client
[1;36m(APIServer pid=1193150)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=1193150)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1193150)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 221, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=1193150)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=1193150)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 1589, in inner
[1;36m(APIServer pid=1193150)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 212, in from_vllm_config
[1;36m(APIServer pid=1193150)[0;0m     return cls(
[1;36m(APIServer pid=1193150)[0;0m            ^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 136, in __init__
[1;36m(APIServer pid=1193150)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=1193150)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=1193150)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=1193150)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 769, in __init__
[1;36m(APIServer pid=1193150)[0;0m     super().__init__(
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
[1;36m(APIServer pid=1193150)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=1193150)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=1193150)[0;0m     next(self.gen)
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 729, in launch_core_engines
[1;36m(APIServer pid=1193150)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=1193150)[0;0m   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 782, in wait_for_engine_startup
[1;36m(APIServer pid=1193150)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=1193150)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
vLLM server exited with status 1
🚀 Launching vLLM server with command:
/home/stewa/code/imageworks/.venv/bin/python3 -m vllm.entrypoints.openai.api_server --model /home/stewa/ai-models/weights/llava-hf/LLaVA-NeXT-Video-7B-hf --host 0.0.0.0 --port 24005 --served-model-name llava-next-video-7b --tensor-parallel-size 1 --gpu-memory-utilization 0.83 --max-model-len 4096 --dtype auto --enforce-eager --trust-remote-code --chat-template llava15_vicuna.jinja
