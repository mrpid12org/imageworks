nohup: ignoring input
INFO 09-26 22:38:18 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:19 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:19 [utils.py:328] non-default args: {'model_tag': './models/Qwen2-VL-2B-Instruct', 'host': '0.0.0.0', 'model': './models/Qwen2-VL-2B-Instruct', 'trust_remote_code': True, 'max_model_len': 4096, 'served_model_name': ['Qwen2-VL-2B-Instruct'], 'gpu_memory_utilization': 0.8}
[1;36m(APIServer pid=995849)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:23 [__init__.py:742] Resolved architecture: Qwen2VLForConditionalGeneration
[1;36m(APIServer pid=995849)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:23 [__init__.py:1815] Using max model len 4096
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:23 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 09-26 22:38:25 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:27 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:27 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='./models/Qwen2-VL-2B-Instruct', speculative_config=None, tokenizer='./models/Qwen2-VL-2B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen2-VL-2B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=995968)[0;0m WARNING 09-26 22:38:27 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[W926 22:38:28.260269613 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:28 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=995968)[0;0m WARNING 09-26 22:38:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=995968)[0;0m WARNING 09-26 22:38:29 [profiling.py:280] The sequence length (4096) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:29 [gpu_model_runner.py:2338] Starting to load model ./models/Qwen2-VL-2B-Instruct...
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:29 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=995968)[0;0m WARNING 09-26 22:38:29 [cuda.py:217] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:29 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=995968)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=995968)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.87it/s]
[1;36m(EngineCore_DP0 pid=995968)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.87it/s]
[1;36m(EngineCore_DP0 pid=995968)[0;0m
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:30 [default_loader.py:268] Loading weights took 0.64 seconds
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:30 [gpu_model_runner.py:2392] Model loading took 4.1513 GiB and 0.814601 seconds
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:30 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:44 [backends.py:539] Using cache directory: /home/stewa/.cache/vllm/torch_compile_cache/b8bb6c0313/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:44 [backends.py:550] Dynamo bytecode transform time: 3.07 s
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.923 s
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:47 [monitor.py:34] torch.compile takes 3.07 s in total
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:48 [gpu_worker.py:298] Available KV cache memory: 5.97 GiB
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:48 [kv_cache_utils.py:864] GPU KV cache size: 223,712 tokens
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:48 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 54.62x
[1;36m(EngineCore_DP0 pid=995968)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:02, 27.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 6/67 [00:00<00:02, 27.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:00<00:02, 28.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:01, 27.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:00<00:01, 27.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:00<00:01, 27.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:00<00:01, 28.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 24/67 [00:00<00:01, 27.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:00<00:01, 27.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:01<00:01, 25.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:01<00:01, 25.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:01<00:01, 26.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:01<00:01, 26.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:00, 28.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 46/67 [00:01<00:00, 27.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 49/67 [00:01<00:00, 27.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:01<00:00, 28.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 55/67 [00:02<00:00, 27.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:02<00:00, 27.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|█████████▎| 62/67 [00:02<00:00, 28.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▊| 66/67 [00:02<00:00, 29.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 27.61it/s]
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:51 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 4.06 GiB
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:51 [gpu_worker.py:391] Free memory on device (14.66/15.99 GiB) on startup. Desired GPU memory utilization is (0.8, 12.79 GiB). Actual usage is 4.15 GiB for weight, 2.65 GiB for peak activation, 0.02 GiB for non-torch memory, and 4.06 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=1893027942` to fit into requested memory, or `--kv-cache-memory=3897485824` to fully utilize gpu memory. Current kv cache memory in use is 6414487654 bytes.
[1;36m(EngineCore_DP0 pid=995968)[0;0m INFO 09-26 22:38:51 [core.py:218] init engine (profile, create kv cache, warmup model) took 20.95 seconds
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 13982
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [api_server.py:1692] Supported_tasks: ['generate']
[1;36m(APIServer pid=995849)[0;0m WARNING 09-26 22:38:52 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.01, 'top_k': 1, 'top_p': 0.001}
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.01, 'top_k': 1, 'top_p': 0.001}
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.01, 'top_k': 1, 'top_p': 0.001}
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [api_server.py:1971] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:38:52 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=995849)[0;0m INFO:     Started server process [995849]
[1;36m(APIServer pid=995849)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=995849)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47678 - "GET /health HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:50:06 [chat_utils.py:538] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:50:12 [loggers.py:123] Engine 000: Avg prompt throughput: 334.9 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:50:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:50:32 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:55:42 [loggers.py:123] Engine 000: Avg prompt throughput: 314.2 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 2.0%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:55:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.0%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:56:02 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.0%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:48350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:59:42 [loggers.py:123] Engine 000: Avg prompt throughput: 177.8 tokens/s, Avg generation throughput: 30.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.7%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 22:59:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.7%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:00:42 [loggers.py:123] Engine 000: Avg prompt throughput: 168.5 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.8%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:00:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.8%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:01:02 [loggers.py:123] Engine 000: Avg prompt throughput: 168.5 tokens/s, Avg generation throughput: 18.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 15.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:01:12 [loggers.py:123] Engine 000: Avg prompt throughput: 147.1 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 14.5%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:01:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 14.5%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:04:52 [loggers.py:123] Engine 000: Avg prompt throughput: 168.9 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 13.0%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:05:02 [loggers.py:123] Engine 000: Avg prompt throughput: 147.6 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.0%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:05:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.0%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:13:32 [loggers.py:123] Engine 000: Avg prompt throughput: 326.5 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.8%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:13:42 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.8%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:14:22 [loggers.py:123] Engine 000: Avg prompt throughput: 302.2 tokens/s, Avg generation throughput: 38.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.9%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:14:32 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:15:42 [loggers.py:123] Engine 000: Avg prompt throughput: 329.4 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:15:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:17:02 [loggers.py:123] Engine 000: Avg prompt throughput: 329.4 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 10.4%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:17:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.4%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:17:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.4%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:18:02 [loggers.py:123] Engine 000: Avg prompt throughput: 330.3 tokens/s, Avg generation throughput: 29.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 9.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:18:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.9%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:18:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.9%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:44640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:20:32 [loggers.py:123] Engine 000: Avg prompt throughput: 330.1 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.7%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:20:42 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.7%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:48264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:21:12 [loggers.py:123] Engine 000: Avg prompt throughput: 290.7 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 9.2%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:48266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:21:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.2%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:21:32 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 9.2%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:33:02 [loggers.py:123] Engine 000: Avg prompt throughput: 425.4 tokens/s, Avg generation throughput: 40.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 8.6%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:33:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 8.6%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:47970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:37:12 [loggers.py:123] Engine 000: Avg prompt throughput: 543.9 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.2%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:37:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 12.2%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:42:12 [loggers.py:123] Engine 000: Avg prompt throughput: 298.8 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 13.2%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:42:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 13.2%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:43:02 [loggers.py:123] Engine 000: Avg prompt throughput: 82.4 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 14.6%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:46544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:43:12 [loggers.py:123] Engine 000: Avg prompt throughput: 208.6 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 17.8%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:43:22 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 17.8%
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO:     127.0.0.1:45124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:46:02 [loggers.py:123] Engine 000: Avg prompt throughput: 291.0 tokens/s, Avg generation throughput: 28.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 22.0%
[1;36m(APIServer pid=995849)[0;0m INFO 09-26 23:46:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 22.0%
