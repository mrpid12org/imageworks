/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
DEBUG 09-30 20:12:54 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 09-30 20:12:54 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 09-30 20:12:54 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 09-30 20:12:54 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 09-30 20:12:54 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 09-30 20:12:54 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 09-30 20:12:54 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 09-30 20:12:55 [entrypoints/utils.py:168] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 09-30 20:12:55 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 09-30 20:12:55 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 09-30 20:12:55 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:12:55 [entrypoints/openai/api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:12:55 [entrypoints/utils.py:328] non-default args: {'model_tag': 'ybelkada/llava-1.5-7b-hf-awq', 'host': '0.0.0.0', 'port': 24001, 'chat_template': 'llava15_vicuna.jinja', 'model': 'ybelkada/llava-1.5-7b-hf-awq', 'dtype': 'float16', 'max_model_len': 4096, 'quantization': 'awq'}
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:01 [config/__init__.py:742] Resolved architecture: LlavaForConditionalGeneration
[1;36m(APIServer pid=970405)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:01 [config/__init__.py:1815] Using max model len 4096
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:01 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:01 [model_executor/.../quantization/awq_marlin.py:121] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:01 [config/__init__.py:1217] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:01 [engine/arg_utils.py:1736] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:01 [engine/arg_utils.py:1745] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:01 [config/scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=970405)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(APIServer pid=970405)[0;0m   warnings.warn(msg, FutureWarning)
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
DEBUG 09-30 20:13:05 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 09-30 20:13:05 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 09-30 20:13:05 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 09-30 20:13:05 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 09-30 20:13:05 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 09-30 20:13:05 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 09-30 20:13:05 [platforms/__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:07 [v1/engine/core.py:654] Waiting for init message from front-end.
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:07 [v1/engine/utils.py:856] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [v1/engine/core.py:662] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/bd2da5e7-d3f6-4858-aded-ee7a9c0ef009'], outputs=['ipc:///tmp/f3dfade1-08c3-4140-90d9-93e80e413d80'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [v1/engine/core.py:494] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:07 [v1/engine/core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ybelkada/llava-1.5-7b-hf-awq', speculative_config=None, tokenizer='ybelkada/llava-1.5-7b-hf-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ybelkada/llava-1.5-7b-hf-awq, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=970570)[0;0m WARNING 09-30 20:13:07 [platforms/interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [utils/__init__.py:3126] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x735159f3b0e0>
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [config/__init__.py:3769] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [config/__init__.py:3771] disabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [distributed/parallel_state.py:988] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.88.229:47787 backend=nccl
[W930 20:13:07.809825955 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:07 [distributed/parallel_state.py:1040] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:07 [distributed/parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=970570)[0;0m /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.
[1;36m(EngineCore_DP0 pid=970570)[0;0m   warnings.warn(msg, FutureWarning)
[1;36m(EngineCore_DP0 pid=970570)[0;0m WARNING 09-30 20:13:08 [v1/sample/ops/topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:08 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=970570)[0;0m Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]
[1;36m(EngineCore_DP0 pid=970570)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3769] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3771] disabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:10 [v1/worker/gpu_model_runner.py:2338] Starting to load model ybelkada/llava-1.5-7b-hf-awq...
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:10 [v1/worker/gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:10 [platforms/cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [compilation/backends.py:36] Using InductorStandaloneAdaptor
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3769] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3771] disabled custom ops: Counter({'column_parallel_linear': 111, 'row_parallel_linear': 111, 'rms_norm': 65, 'silu_and_mul': 32, 'quick_gelu': 1, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3769] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [config/__init__.py:3771] disabled custom ops: Counter({'column_parallel_linear': 111, 'row_parallel_linear': 111, 'rms_norm': 65, 'silu_and_mul': 32, 'quick_gelu': 1, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:10 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:11 [model_executor/model_loader/weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:11 [model_executor/model_loader/weight_utils.py:406] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=970570)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:11 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:11 [model_executor/models/utils.py:183] Loaded weight multi_modal_projector.linear_1.bias with shape torch.Size([4096])
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:11 [model_executor/models/utils.py:183] Loaded weight multi_modal_projector.linear_1.weight with shape torch.Size([4096, 1024])
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:11 [model_executor/models/utils.py:183] Loaded weight multi_modal_projector.linear_2.bias with shape torch.Size([4096])
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:11 [model_executor/models/utils.py:183] Loaded weight multi_modal_projector.linear_2.weight with shape torch.Size([4096, 4096])
[1;36m(EngineCore_DP0 pid=970570)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]
[1;36m(EngineCore_DP0 pid=970570)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]
[1;36m(EngineCore_DP0 pid=970570)[0;0m
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:11 [model_executor/model_loader/default_loader.py:268] Loading weights took 0.65 seconds
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:12 [v1/worker/gpu_model_runner.py:2392] Model loading took 4.2532 GiB and 1.461066 seconds
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:12 [v1/worker/gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 3 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:12 [compilation/decorators.py:254] Start compiling function <code object forward at 0x22b08010, file "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/_custom_ops.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:15 [compilation/backends.py:492] /home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:15 [compilation/backends.py:539] Using cache directory: /home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:15 [compilation/backends.py:550] Dynamo bytecode transform time: 3.36 s
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_0')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 1-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_1', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_1')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 2-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_2', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_2')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 3-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_3', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_3')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 4-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_4', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_4')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 5-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_5', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_5')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 6-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_6', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_6')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 7-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_7', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_7')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 8-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_8', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_8')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 9-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_9', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_9')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 10-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_10', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_10')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 11-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_11', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_11')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 12-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_12', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_12')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 13-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_13', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_13')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 14-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_14', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_14')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 15-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_15', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_15')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 16-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_16', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_16')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 17-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_17', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_17')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 18-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_18', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_18')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:16 [compilation/backends.py:124] Directly load the 19-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_19', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_19')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 20-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_20', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_20')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 21-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_21', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_21')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 22-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_22', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_22')
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:17 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 23-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_23', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_23')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 24-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_24', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_24')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 25-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_25', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_25')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 26-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_26', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_26')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 27-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_27', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_27')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_28')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 29-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_29', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_29')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 30-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_30', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_30')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 31-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_31', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_31')
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:17 [compilation/backends.py:124] Directly load the 32-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_32', '/home/stewa/.cache/vllm/torch_compile_cache/37c289977b/rank_0_0/backbone/artifact_shape_None_subgraph_32')
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:17 [compilation/backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.385 s
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:17 [compilation/monitor.py:34] torch.compile takes 3.36 s in total
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:18 [v1/worker/gpu_worker.py:284] Initial free memory: 14.66 GiB; Requested memory: 0.90 (util), 14.39 GiB
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:18 [v1/worker/gpu_worker.py:291] Free memory after profiling: 10.29 GiB (total), 10.03 GiB (within requested)
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:18 [v1/worker/gpu_worker.py:297] Memory profiling takes 6.52 seconds. Total non KV cache memory: 4.61GiB; torch peak memory increase: 0.34GiB; non-torch forward increase memory: 0.02GiB; weights memory: 4.25GiB.
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:18 [v1/worker/gpu_worker.py:298] Available KV cache memory: 9.78 GiB
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:19 [v1/core/kv_cache_utils.py:864] GPU KV cache size: 20,032 tokens
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:19 [v1/core/kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 4.89x
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [config/__init__.py:3769] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [config/__init__.py:3771] disabled custom ops: Counter({'column_parallel_linear': 111, 'row_parallel_linear': 111, 'rms_norm': 65, 'silu_and_mul': 32, 'quick_gelu': 1, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=970570)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|▏         | 1/67 [00:00<00:11,  5.54it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 2/67 [00:00<00:11,  5.82it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:09,  6.49it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:09,  6.69it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 5/67 [00:00<00:09,  6.24it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 6/67 [00:00<00:09,  6.45it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|█         | 7/67 [00:01<00:09,  6.51it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:01<00:09,  6.49it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:01<00:08,  6.59it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|█▍        | 10/67 [00:01<00:08,  6.68it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▋        | 11/67 [00:01<00:08,  6.73it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:01<00:08,  6.86it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:01<00:07,  6.95it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 14/67 [00:02<00:07,  6.77it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:02<00:07,  6.84it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 16/67 [00:02<00:07,  7.00it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:02<00:06,  7.18it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:02<00:06,  7.28it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:02<00:06,  7.38it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|██▉       | 20/67 [00:03<00:18,  2.50it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:03<00:14,  3.15it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 22/67 [00:04<00:11,  3.80it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:04<00:09,  4.49it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 24/67 [00:04<00:08,  5.11it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:04<00:07,  5.75it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 26/67 [00:04<00:06,  6.24it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:04<00:06,  6.50it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:04<00:05,  6.73it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 29/67 [00:04<00:05,  6.92it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:05<00:05,  7.15it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:05<00:05,  6.88it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|████▊     | 32/67 [00:05<00:05,  6.54it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:05<00:05,  6.68it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 34/67 [00:05<00:04,  6.71it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|█████▏    | 35/67 [00:05<00:04,  6.76it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:06<00:04,  6.55it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 37/67 [00:06<00:04,  6.76it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 38/67 [00:06<00:04,  6.72it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:06<00:04,  5.79it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|█████▉    | 40/67 [00:06<00:04,  6.22it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 41/67 [00:06<00:04,  6.50it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 42/67 [00:06<00:03,  6.57it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:07<00:03,  6.78it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 44/67 [00:07<00:03,  6.89it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 45/67 [00:07<00:03,  6.85it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 46/67 [00:07<00:02,  7.03it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:26 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|███████   | 47/67 [00:07<00:02,  7.05it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:07<00:02,  7.11it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 49/67 [00:07<00:02,  7.31it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 50/67 [00:08<00:02,  7.48it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▌  | 51/67 [00:08<00:02,  7.48it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:08<00:01,  7.58it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:08<00:01,  7.56it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 54/67 [00:08<00:01,  7.82it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:27 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 55/67 [00:08<00:01,  7.99it/s][1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▎ | 56/67 [00:08<00:01,  8.40it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 57/67 [00:08<00:01,  8.04it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:09<00:01,  7.90it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 59/67 [00:09<00:01,  7.86it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|████████▉ | 60/67 [00:09<00:00,  7.92it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:09<00:00,  7.64it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|█████████▎| 62/67 [00:09<00:00,  7.95it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:28 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:09<00:00,  8.07it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:29 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 64/67 [00:09<00:00,  8.09it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:29 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 65/67 [00:09<00:00,  8.02it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:29 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▊| 66/67 [00:10<00:00,  7.87it/s][1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:29 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:10<00:00,  6.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:10<00:00,  6.52it/s]
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:29 [v1/worker/gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.48 GiB
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:29 [v1/worker/gpu_worker.py:391] Free memory on device (14.66/15.99 GiB) on startup. Desired GPU memory utilization is (0.9, 14.39 GiB). Actual usage is 4.25 GiB for weight, 0.34 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.48 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=9831288627` to fit into requested memory, or `--kv-cache-memory=10118650880` to fully utilize gpu memory. Current kv cache memory in use is 10504474419 bytes.
[1;36m(EngineCore_DP0 pid=970570)[0;0m INFO 09-30 20:13:29 [v1/engine/core.py:218] init engine (profile, create kv cache, warmup model) took 17.59 seconds
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:30 [v1/engine/utils.py:856] READY from local core engine process 0.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:30 [v1/engine/core.py:747] EngineCore waiting for work.
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:30 [v1/metrics/loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1252
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:30 [v1/engine/async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:30 [v1/engine/core.py:747] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:13:30 [v1/engine/core.py:747] EngineCore waiting for work.
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:30 [entrypoints/openai/api_server.py:1692] Supported_tasks: ['generate']
[1;36m(APIServer pid=970405)[0;0m Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]
[1;36m(APIServer pid=970405)[0;0m Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 2587.48it/s]
[1;36m(APIServer pid=970405)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474] Failed to load AutoTokenizer chat template for ybelkada/llava-1.5-7b-hf-awq
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474] Traceback (most recent call last):
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py", line 472, in resolve_hf_chat_template
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474]     return tokenizer.get_chat_template(chat_template, tools=tools)
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474]   File "/home/stewa/code/imageworks/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 1798, in get_chat_template
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474]     raise ValueError(
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:474] ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:32 [entrypoints/chat_utils.py:493] There is no chat template fallback for ybelkada/llava-1.5-7b-hf-awq
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] Using supplied chat template: {# llava15_vicuna.jinja #}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% set image_token = "<image>" %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% for m in messages %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% if m['role'] == 'system' %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] SYSTEM: {{ m['content'] | trim }}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% elif m['role'] == 'user' %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] USER:
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {%- if m['content'] is string -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]   {{ m['content'] | trim }}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {%- else -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]   {# handle OpenAI Vision content blocks #}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]   {%- for part in m['content'] -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]     {%- if part['type'] == 'text' -%}{{ part['text'] | trim }}{% endif -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]     {%- if part['type'] == 'image_url' -%} {{ image_token }} {% endif -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712]   {%- endfor -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {%- endif -%}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% elif m['role'] == 'assistant' %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] ASSISTANT: {{ m['content'] | trim }}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% endif %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] {% endfor %}
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] ASSISTANT:
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:13:32 [entrypoints/openai/api_server.py:1712] It is different from official chat template 'ybelkada/llava-1.5-7b-hf-awq'. This discrepancy may lead to performance degradation.
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/openai/api_server.py:1971] Starting vLLM API server 0 on http://0.0.0.0:24001
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:36] Available routes are:
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:13:33 [entrypoints/launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=970405)[0;0m INFO:     Started server process [970405]
[1;36m(APIServer pid=970405)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=970405)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:43 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:52 [v1/engine/async_llm.py:624] Called check_health.
[1;36m(APIServer pid=970405)[0;0m INFO:     192.168.88.229:47360 - "GET /health HTTP/1.1" 200 OK
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:13:53 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:14:04 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:14:10 [entrypoints/chat_utils.py:538] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=970405)[0;0m Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 26715.31it/s]
[1;36m(APIServer pid=970405)[0;0m INFO:     192.168.88.229:48360 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:14:14 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:14:24 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m DEBUG 09-30 20:14:35 [v1/metrics/loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:14:42 [entrypoints/launcher.py:98] port 24001 is used by process psutil.Process(pid=970405, name='vllm', status='running') launched with command:
[1;36m(APIServer pid=970405)[0;0m WARNING 09-30 20:14:42 [entrypoints/launcher.py:98] /home/stewa/code/imageworks/.venv/bin/python /home/stewa/code/imageworks/.venv/bin/vllm serve ybelkada/llava-1.5-7b-hf-awq --host 0.0.0.0 --port 24001 --quantization awq --dtype float16 --chat-template llava15_vicuna.jinja --max-model-len 4096
[1;36m(APIServer pid=970405)[0;0m INFO 09-30 20:14:42 [entrypoints/launcher.py:101] Shutting down FastAPI HTTP server.
[1;36m(EngineCore_DP0 pid=970570)[0;0m DEBUG 09-30 20:14:42 [v1/engine/core.py:714] EngineCore exiting.
[rank0]:[W930 20:14:42.215674808 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=970405)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=970405)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=970405)[0;0m INFO:     Application shutdown complete.
