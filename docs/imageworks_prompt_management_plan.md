# ImageWorks — Unified Prompt Management & Telemetry with Proxy-Orchestrated Backends

## 0) Executive Summary

- **What you’ll build:**
  A shared **Prompt Hub** + **centralised telemetry** layer integrated into your **chat-proxy** (orchestrator), which routes requests to **vLLM**, **Ollama**, and **TF-IQA** (and later **TensorRT-LLM**) backends using your **Model Registry** as the single source of truth.

- **Why:**
  Today, Judge-Vision Stage-1 (IQA) bypasses the proxy; prompts are app-local; vLLM shares a container with the proxy; and telemetry is fragmented. The new design makes the proxy the one URL and one source for observability, while prompts become reusable, versioned assets shared across apps.

- **Key deliverables:**
  1) Split **vLLM** out of the chat-proxy container.
  2) Add `/v1/iqa/run` in the proxy; wrap your **TF-IQA** microservice as a managed backend.
  3) Introduce `imageworks/prompt_hub` for shared prompt storage, loading, inheritance, and rendering.
  4) Make the **Model Registry** power proxy routing + parameter mapping to each provider.
  5) Add **run manifest** + **per-request JSONL** telemetry (performance + quality) for Judge-Vision and Personal-Tagger.
  6) Provide tests, CI checks, and a smooth migration plan.

- **Inspirations (design patterns):**
  - **LangChain/LangGraph** for prompt templating and graph-based orchestration patterns
  - **Haystack PromptNode** for YAML/JSON prompt registries
  - **PromptLayer/PromptHub** for prompt/version tagging and experiment hygiene
  - **DSPy** for offline prompt optimization over labelled datasets
  - **promptfoo** for prompt CI tests

## 14) Implementation Plan (Detailed)

- **Stage 1 — Container & Deployment Split (COMPLETED)**
  - `Dockerfile.vllm` now builds the standalone **imageworks-vllm** executor (ports `8600/v1`), and the chat-proxy reaches it via `CHAT_PROXY_VLLM_BASE_URL` (defaults to `http://imageworks-vllm:8600`). GPU leases remain enforced inside the proxy, but vLLM spins up/down independently so Judge Vision Stage 2 no longer blocks on proxy restarts.
  - The proxy container (`imageworks-chat-proxy`) sheds vLLM dependencies and only ships FastAPI + routing + orchestration logic. `docs/runbooks/chat-proxy.md` documents the rollout plus `docker compose -f docker-compose.chat-proxy.yml up -d --force-recreate chat-proxy` as the restart path when Python changes land.
  - TF-IQA already lives in `imageworks-tf-iqa-service` (built from `Dockerfile.tf-iqa`) with `/healthz` + `/infer` endpoints; Stage 1 recorded those contracts so Stage 3 can wrap them through the proxy.
  - Ollama coexistence is handled at the proxy layer: GPU lease acquisition now calls the new keep-alive unload (see `src/imageworks/chat_proxy/ollama_manager.py`) so models such as `gemma-3-27b` drop from VRAM before vLLM boots. This resolved the long tail where Stage 2 sat idle waiting for a non-existent `/api/stop`.

- **Stage 2 — Prompt Hub Foundations**
  - Create `src/imageworks/prompt_hub/` with schemas (PromptProfile, StageTemplate, metadata hash) and load order (base → app → user overrides).
  - Port Personal Tagger prompts to the hub (read-only at first) and mirror Judge Vision prompts into the same format.
  - Ship CLI helpers (`python -m imageworks.prompt_hub list|inspect|export`) for editing until a GUI exists.
  - Update telemetry schemas to include `prompt_profile_id`, `profile_hash`, `prompt_hub_version`; add unit tests for load/override/hash behavior.

- **Stage 3 — Proxy-Orchestrated TF Backend**
  - Extend model registry schema with capability blocks (`chat`, `iqa`, `vision`) describing connection info, GPU/memory footprint, and compatibility hints.
  - Add `/v1/iqa/run` to the proxy: validate requests, acquire GPU leases, forward to TF service, emit telemetry (request id, latency, backend id).
  - Update Judge Vision runner to call the proxy instead of invoking Docker directly; capture per-image JSONL telemetry generated by the proxy.

- **Stage 4 — Registry-Driven Orchestration & Prompt Adoption**
  - Wire Judge Vision and Personal Tagger to load prompts exclusively via Prompt Hub (config chooses profile id/name).
  - Update GUI/CLI selectors to list profiles from the hub.
  - Teach the proxy to resolve backend targets via the registry, ensuring each request specifies a logical model while the proxy enforces compatibility and emits clear errors when entries are missing.

- **Stage 5 — Telemetry & Performance Surfaces**
  - Implement a run manifest capturing git SHA, prompt profiles, registry versions, backend selections, and env flags.
  - Emit per-stage/per-image metrics: enqueue time, backend latency, GPU lease waits, retry counts, payload sizes. Store JSONL logs plus aggregated summaries.
  - Add a Judge Vision GUI “Performance” tab that renders the latest manifest + telemetry, exposing throughput, error rates, and backend utilization.
  - Provide CLI tooling to summarize telemetry artifacts.

- **Stage 6 — Forward-Looking Multi-Model Capacity**
  - Expand registry entries with VRAM footprint, preferred GPU partition (full card vs MIG slice), and coexistence tags.
  - Enhance the proxy scheduler to track active allocations and (initially) static VRAM accounting; later integrate live NVML sampling.
  - Define policies for loading/evicting backends based on available VRAM and compatibility. Log “would block” decisions during Stage 6 even before automatic partitioning ships.

- **Stage 7 — Prompt Editing UX (post-MVP)**
  - Build a central GUI editor (likely Streamlit/FastAPI) that manipulates Prompt Hub data, with revision tagging and access controls.
  - Provide reload hooks (`/v1/prompt/reload`) so apps pick up edits without restarts.

- **Stage 8 — QA, Docs, and Rollout**
  - Author migration guides for ops (new containers, env vars, telemetry storage) and devs (adding prompt profiles, interpreting metrics).
  - Extend CI with prompt hub, proxy routing, registry schema, and telemetry emission tests.
  - Pilot with Judge Vision, then Personal Tagger, before rolling to other modules; keep feature flags/config switches for gradual adoption.

... (full document continues exactly as written above, with all sections 0–13) ...
